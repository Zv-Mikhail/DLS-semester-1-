# Homework 1 — Game of Thrones Character Survival Prediction

В данном задании анализировался датасет о персонажах из вселенной Game of Thrones (по данным “A Wiki of Ice and Fire”).  
Цель — предсказать, кто из персонажей останется в живых, а кто погибнет.

## Exploratory Data Analysis (EDA)

В ходе анализа данных были выполнены следующие шаги:
- Изучено распределение признаков, связанных с происхождением, родом, статусом и участием в войнах.  
- Проведен анализ пропусков и категориальных переменных.  
- Построены корреляционные матрицы и визуализации зависимостей между признаками и целевой переменной.  
- Выявлены признаки, между которыми присутствуют нелинейные зависимости.

## Модель

В качестве базовой модели использовалась логистическая регрессия.  
Модель обучалась на очищенных и нормализованных данных, включая бинаризацию категориальных признаков.

**Метрика:** Accuracy  
**Результат:** 0.69

## Выводы

По результатам EDA было замечено, что в данных присутствуют нелинейные зависимости, однако в рамках данного задания требовалось использовать именно логистическую регрессию.  
Полученная точность 0.69 является разумным результатом для базового линейного классификатора.


# Homework 2 — Реализация логистической регрессии с SGD

В данном задании требовалось реализовать собственный класс `MyLogisticRegression` с поддержкой **L1 и L2 регуляризаций**, а также оптимизацией с помощью **стохастического градиентного спуска (SGD)**.  

Были написаны вспомогательные функции:
- `grad_f` и `grad_descent_2d` для расчета и проверки градиентов,  
- `generate_batches` для формирования батчей,  
- `fit` и `get_grad` как методы обучения и вычисления градиента внутри модели.  

Задание направлено на закрепление понимания принципов обучения логистической регрессии и работы градиентных методов оптимизации.


# Homework 3 — Предсказание оттока пользователей (Kaggle Competition)

В этом домашнем задании была поставлена задача **предсказания оттока клиентов**. Цель — построить модель машинного обучения, которая сможет предсказывать вероятность того, что клиент перестанет пользоваться сервисом.  

## Exploratory Data Analysis (EDA)

Для изучения данных была проведена подробная разведка:
- Использована библиотека `ydata-profiling` для быстрого и наглядного анализа датасета, включая распределения признаков, пропуски, корреляции и потенциальные выбросы.  
- Проведен анализ количественных и категориальных признаков, выявлены закономерности и зависимости с целевой переменной.  
- Выполнена очистка данных, обработка пропусков и кодирование категориальных переменных для последующего обучения моделей.  

## Модели

В работе использовались несколько подходов:

1. **Логистическая регрессия**
   - После тщательной подготовки данных логистическая регрессия показала **ROC-AUC = 0.8341**.  
   - Модель служила базовым ориентиром для последующих экспериментов.

2. **Градиентный бустинг**
   - Применены различные реализации: XGBoost, LightGBM, CatBoost.  
   - Использован pipeline и **Bayesian optimization** для подбора гиперпараметров.  
   - Наилучший результат — **ROC-AUC = 0.8546**.  

3. **AutoML (FLAML)**
   - Попробована автоматическая подборка моделей с использованием `from flaml import AutoML`.  
   - AutoML помог выявить потенциально эффективные комбинации моделей и параметров без ручного перебора.  

## Выводы

- Правильная подготовка данных оказала ключевое влияние на результат: как для линейной модели, так и для бустинговых алгоритмов.  
- Бустинговые модели с оптимизацией гиперпараметров показали лучший результат по сравнению с простой логистической регрессией.  
- Использование AutoML позволяет ускорить процесс поиска эффективной модели, особенно на больших и сложных датасетах.  
- В целом, проделанная работа закрепила навыки EDA, построения пайплайнов, работы с бустингом и AutoML, а также подготовки данных для ML-конкурсов.  

**Результат на Kaggle:** [207 место из 5724 участников](https://www.kaggle.com/competitions/advanced-dls-spring-2021/leaderboard)  
**Team:** Звягинцев Михаил

# Homework 4 — Введение в Deep Learning и PyTorch

В этом задании основной целью было **погружение в основы глубокого обучения** и освоение библиотеки **PyTorch** на практике.  
Работа включала реализацию нейронных сетей "с нуля" и исследование влияния различных архитектур и функций на качество обучения.

## Основные этапы работы

1. **Подготовка данных**  
   - Загрузка и предобработка нескольких датасетов (в том числе изображений).  
   - Разделение данных на обучающую и тестовую выборки.  
   - Создание `DataLoader` для эффективной итерации по батчам.

2. **Создание моделей**  
   - Реализован собственный класс модели на базе `torch.nn.Module`.  
   - Изучены и протестированы базовые архитектуры полносвязных и сверточных нейронных сетей.  
   - Проведены эксперименты с различной глубиной сети, количеством нейронов и функциями активации.

3. **Функции обучения и тестирования**  
   - Реализованы функции для обучения модели на тренировочных данных и оценки результатов на тестовой выборке.  
   - Добавлено отслеживание метрик (accuracy, loss) и визуализация динамики обучения.

4. **Изучение свёрточных сетей (CNN)**  
   - Реализованы и протестированы простые свёрточные архитектуры.  
   - Изучено влияние различных типов свёрток, размеров ядер и паддинга на качество распознавания изображений.  
   - Проведены эксперименты с изменением глубины и количества фильтров.

5. **Функции активации**  
   - Реализованы и протестированы несколько функций активации (ReLU, Sigmoid, Tanh, LeakyReLU и др.).  
   - Сравнено их влияние на скорость сходимости и качество обучения модели.

## Выводы

- Работа позволила закрепить понимание ключевых этапов построения нейронных сетей: от подготовки данных до обучения и оценки.  
- PyTorch показал гибкость и удобство в определении архитектур и управлении процессом обучения.  
- Эксперименты с различными функциями активации и типами свёрток помогли глубже понять внутреннюю механику работы нейросетей.  
- Задание стало прочной основой для дальнейшего изучения глубокого обучения и перехода к более сложным моделям.


# Homework 5 — Классификация персонажей Simpsons (Kaggle Competition)

В этом домашнем задании решалась задача **классификации персонажей сериала “The Simpsons”** по изображениям.  
Необходимо было построить устойчивую и точную модель, способную распознавать более **40 классов персонажей**.

## Описание задачи

Датасет содержал изображения персонажей сериала *The Simpsons*, распределённые по классам. Цель — построить модель, способную корректно классифицировать персонажа по фотографии.  
Работа выполнялась в формате соревнования на Kaggle, где оценка производилась по метрике **accuracy**.

## Подход и используемые модели

Для решения задачи применялись предобученные архитектуры и методы **transfer learning**, а также различные техники регуляризации и ансамблирования.

---

### 1. ResNet50
- В качестве базовой модели использована **ResNet50**, предобученная на ImageNet.  
- Финальный слой `fc` был заменён на собственную "голову" модели:
  ```
  Linear → BatchNorm → ReLU → Dropout → Linear(num_classes)
  ```
- Заморожены ранние слои бэкбона, обучалась только новая голова.  
- Оптимизатор: `AdamW`  
- Планировщик: `CosineAnnealingLR`  
- Функция потерь: `CrossEntropyLoss`

**Результат:** Accuracy = `0.99468`

---

### 2. ResNet50 + MixUp
- Добавлена техника **MixUp** для регуляризации и повышения обобщающей способности модели.  
- Реализовано сохранение лучших весов по валидационной точности:
  ```python
  if val_acc > best_val_acc:
      best_val_acc = val_acc
      torch.save(model.state_dict(), "best_mixup_model.pth")
  ```
- Добавлена возможность возобновления обучения с сохранённых весов.

**Результат:** Accuracy = `0.9681`

---

### 3. InceptionV3 + Ensemble
- В качестве третьего подхода использована **InceptionV3**, также предобученная на ImageNet.  
- Для улучшения итоговой метрики применено **взвешенное ансамблирование** трёх моделей:
  - ResNet50  
  - ResNet50 + MixUp  
  - InceptionV3  
- Усреднение предсказаний проводилось с весами, пропорциональными точности каждой модели.

**Финальный результат:** Accuracy = `0.99893`  
(всего одна ошибка на всём тестовом наборе)

---

## Технические детали
- Реализована система логирования обучения и визуализация кривых потерь и точности.  
- Использованы приёмы data augmentation (flip, rotation, color jitter).  
- Обучение проводилось с использованием GPU (MPS / CUDA).  
- Добавлено раннее прекращение (early stopping) при отсутствии улучшений метрики.  

---

## Выводы
- Использование **transfer learning** позволило достичь высокой точности при ограниченном количестве данных.  
- Техника **MixUp** и **ансамблирование моделей** существенно повысили устойчивость и качество итоговых предсказаний.  
- Эксперименты показали важность подбора оптимальной архитектуры и гиперпараметров, а также аккуратного обращения с обучающими данными.  


**Результат на Kaggle:** [3 место из 100+ участников](https://www.kaggle.com/competitions/journey-to-springfield1/leaderboard)  
**Team:** Михаил_Звягинцев_414906596
